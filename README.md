# NC_regression
This is the codebase for Neural Regression Collapse on MuJoCo datasets. The code is focusing on the non-UFM case now, so modifications are needed to run UFM experiments.

## Dataset
Please find the MuJoCo datasets via [this link](https://drive.google.com/file/d/1XScUTYrXsfMCgEQQtURCmUSS0IGxatZX/view?usp=drive_link). The reacher and swimmer datasets come from [this project](https://huggingface.co/datasets/jat-project/jat-dataset), and the hopper dataset is from D4RL. Each dataset is a dictionary with 3 keys: `observations`, `actions`, and `rewards`. The corresponding values are numpy arrays. The data ratio between the training and test split is $9:1$. After downloading the datasets, please put them under `/NC_regression/dataset/mujoco`.

## Run Experiments
For your reference, all codes for experiments with L2 regularization are contained in `/NC_regression/main/BC.py`. To run the codes, one might as well take a look at the launcher files under `/NC_regression/main/exp/case1`, where `.py` files set up experimental hyperparameters, and `.sh` files submit jobs onto HPC. This is an exemplary workflow. One may use the code at their discretion.

### Workflow on Jubail HPC without Singularity
Please follow the instructions below to get familiar with this workflow. You can also find [instructions here](https://crc-docs.abudhabi.nyu.edu/hpc/training/miniconda.html) from the NYUAD HPC website.

1. Modify any Python files/code on your local machine if necessary.
2. Send modified code to your HPC working directory, e.g. `/scratch/your_netid/NC_regression`. This can be done in many ways:
   - Thrid-party app, e.g. FileZilla
   - Github
   - Linux `rsync` command
   - Set up remote development in VScode
3. Create a virtual environment via Conda on HPC.
   ```
   # Load miniconda to your working space. This only needs to be done once
   module load miniconda
   source ~/.bashrc

   # Go to your project folder. $SCRATCH is the environment variable meaning /scratch/your_netid/.
   cd $SCRATCH/NC_regression
   
   # Create a virtual environment from an existing yaml file, e.g. /NC_regression/environment.yml
   conda env create -f environment.yml
   
   # Or you can create one and install whatever packages yourself later
   conda env create -n project_name python=3.9
   conda install numpy
   pip install wandb
   ```
4. Log experiment results via [Weight and Biases](https://wandb.ai/site). The results are automatically logged on WANDB. Sign up if one does not have an account. Remember to follow the instructions there to assign the WANDB API Key to the corresponding environment variable on the HPC. Also, remember to set the project name, group name, and experiment name in the launcher Python files accordingly.
5. Double-check configurations in the SLURM job script, e.g. computation resources, data path, log path, and executable Python files path, and submit it via `sbatch job_script.sh`. Please take a look at `/NC_regression/main/exp/case1/test.sh` to debug your code and `.../reproduce.sh` to reproduce experiments. Remember to change the email address in the script to receive notification about your job's status, and create and set a correct folder to store output and error files generated by SLURM. You can use `squeue -u your_netid` to check the status of your submitted job. You can also check the output and error information via `cat job_ID.out/ cat job_ID.err`.
